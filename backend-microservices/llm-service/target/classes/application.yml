server:
  port: 8086

spring:
  application:
    name: llm-service

# OpenAI Configuration
openai:
  api:
    key: ${OPENAI_API_KEY:}  # Set your OpenAI API key as environment variable
    url: https://api.openai.com/v1/chat/completions
  model: gpt-4  # or gpt-3.5-turbo for cheaper option

eureka:
  client:
    service-url:
      defaultZone: http://localhost:8761/eureka
  instance:
    hostname: localhost

logging:
  level:
    com.mrca.llm: DEBUG
    org.springframework.web.reactive.function.client.ExchangeFunctions: WARN